{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies"
      ],
      "metadata": {
        "id": "Zi65RoJIkMKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install datasets transformers trl peft accelerate bitsandbytes auto-gptq optimum"
      ],
      "metadata": {
        "id": "pwMj-8TYjcw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "biyFiZQWkBgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dependencies"
      ],
      "metadata": {
        "id": "yLfVtVBekXpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, TrainingArguments\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "V1l9lXtFkTpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot Config"
      ],
      "metadata": {
        "id": "KlxU4Y-cl0KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    MODEL_ID = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
        "    DATASET_ID = \"talktolisten/roleplay\"\n",
        "    CONTEXT_FIELD= \"\"\n",
        "    INSTRUCTION_FIELD = \"instruction\"\n",
        "    TARGET_FIELD = \"response\"\n",
        "    BITS = 4\n",
        "    DISABLE_EXLLAMA = True\n",
        "    DEVICE_MAP = \"auto\"\n",
        "    USE_CACHE = False\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 16\n",
        "    LORA_DROPOUT = 0.05\n",
        "    BIAS = \"none\"\n",
        "    TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
        "    TASK_TYPE = \"CAUSAL_LM\"\n",
        "    OUTPUT_DIR = \"ttl-roleplay\"\n",
        "    BATCH_SIZE = 64\n",
        "    GRAD_ACCUMULATION_STEPS = 1\n",
        "    OPTIMIZER = \"paged_adamw_32bit\"\n",
        "    LR = 2e-4\n",
        "    LR_SCHEDULER = \"cosine\"\n",
        "    LOGGING_STEPS = 50\n",
        "    SAVE_STRATEGY = \"epoch\"\n",
        "    NUM_TRAIN_EPOCHS = 1\n",
        "    MAX_STEPS = 5\n",
        "    FP16 = True\n",
        "    PUSH_TO_HUB = True\n",
        "    DATASET_TEXT_FIELD = \"text\"\n",
        "    MAX_SEQ_LENGTH = 512\n",
        "    PACKING = False"
      ],
      "metadata": {
        "id": "ZQp4uYEtl2cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zephyr Trainer"
      ],
      "metadata": {
        "id": "fVxiX-UllcKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ZephyrTrainer:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        '''\n",
        "        A Trainer used to train the Zephyr 7B model which beats Llama2-70b-chat model for your custom usecase\n",
        "\n",
        "        Initialized:\n",
        "        config: Parameters required for the trainer to create and process dataset, train and save model finally\n",
        "        tokenizer: Tokenizer required in training loop\n",
        "        '''\n",
        "\n",
        "        self.config = Config()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.MODEL_ID)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def create_dataset(self):\n",
        "\n",
        "        '''\n",
        "        Downloads and processes the dataset\n",
        "\n",
        "        Returns:\n",
        "        processed_data: Training ready processed dataset\n",
        "        '''\n",
        "\n",
        "        data = load_dataset(self.config.DATASET_ID, split=\"train\")\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tDOWNLOADED DATASET\")\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        df = data.to_pandas()\n",
        "        df['text'] = df['response'].apply(lambda x: x['text'])\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tPROCESSED DATASET\")\n",
        "        print(df.iloc[0])\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        processed_data = Dataset.from_pandas(df[[self.config.DATASET_TEXT_FIELD]])\n",
        "        return processed_data\n",
        "\n",
        "    def prepare_model(self):\n",
        "\n",
        "        '''\n",
        "        Prepares model for finetuning by quantizing it and attaching lora modules to the model\n",
        "\n",
        "        Returns:\n",
        "        model - Model ready for finetuning\n",
        "        peft_config - LoRA Adapter config\n",
        "        '''\n",
        "\n",
        "        bnb_config = GPTQConfig(\n",
        "                                    bits=self.config.BITS,\n",
        "                                    disable_exllama=self.config.DISABLE_EXLLAMA,\n",
        "                                    tokenizer=self.tokenizer\n",
        "                                )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "                                                        self.config.MODEL_ID,\n",
        "                                                        quantization_config=bnb_config,\n",
        "                                                        device_map=self.config.DEVICE_MAP\n",
        "                                                    )\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tDOWNLOADED MODEL\")\n",
        "        print(model)\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        model.config.use_cache=self.config.USE_CACHE\n",
        "        model.config.pretraining_tp=1\n",
        "        model.gradient_checkpointing_enable()\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tMODEL CONFIG UPDATED\")\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        peft_config = LoraConfig(\n",
        "                                    r=self.config.LORA_R,\n",
        "                                    lora_alpha=self.config.LORA_ALPHA,\n",
        "                                    lora_dropout=self.config.LORA_DROPOUT,\n",
        "                                    bias=self.config.BIAS,\n",
        "                                    task_type=self.config.TASK_TYPE,\n",
        "                                    target_modules=self.config.TARGET_MODULES\n",
        "                                )\n",
        "\n",
        "        model = get_peft_model(model, peft_config)\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tPREPARED MODEL FOR FINETUNING\")\n",
        "        print(model)\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        return model, peft_config\n",
        "\n",
        "    def set_training_arguments(self):\n",
        "\n",
        "        '''\n",
        "        Sets the arguments for the training loop in TrainingArguments class\n",
        "        '''\n",
        "\n",
        "        training_arguments = TrainingArguments(\n",
        "                                                output_dir=self.config.OUTPUT_DIR,\n",
        "                                                per_device_train_batch_size=self.config.BATCH_SIZE,\n",
        "                                                gradient_accumulation_steps=self.config.GRAD_ACCUMULATION_STEPS,\n",
        "                                                optim=self.config.OPTIMIZER,\n",
        "                                                learning_rate=self.config.LR,\n",
        "                                                lr_scheduler_type=self.config.LR_SCHEDULER,\n",
        "                                                save_strategy=self.config.SAVE_STRATEGY,\n",
        "                                                logging_steps=self.config.LOGGING_STEPS,\n",
        "                                                num_train_epochs=self.config.NUM_TRAIN_EPOCHS,\n",
        "                                                max_steps=self.config.MAX_STEPS,\n",
        "                                                fp16=self.config.FP16,\n",
        "                                                push_to_hub=self.config.PUSH_TO_HUB\n",
        "                                            )\n",
        "\n",
        "        return training_arguments\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        '''\n",
        "        Trains the model on the specified dataset in config\n",
        "        '''\n",
        "\n",
        "        data = self.create_dataset()\n",
        "        model, peft_config = self.prepare_model()\n",
        "        training_args = self.set_training_arguments()\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tPREPARED FOR FINETUNING\")\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        trainer = SFTTrainer(\n",
        "                                model=model,\n",
        "                                train_dataset=data,\n",
        "                                peft_config=peft_config,\n",
        "                                dataset_text_field=self.config.DATASET_TEXT_FIELD,\n",
        "                                args=training_args,\n",
        "                                tokenizer=self.tokenizer,\n",
        "                                packing=self.config.PACKING,\n",
        "                                max_seq_length=self.config.MAX_SEQ_LENGTH\n",
        "                            )\n",
        "        trainer.train()\n",
        "\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "        print(\"\\t\\t\\tFINETUNING COMPLETED\")\n",
        "        print(\"\\n====================================================================\\n\")\n",
        "\n",
        "        trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "jpj9gBxula4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    zephyr_trainer = ZephyrTrainer()\n",
        "    zephyr_trainer.train()"
      ],
      "metadata": {
        "id": "jUhYereLF380"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import GenerationConfig\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/ttl-roleplay\")\n",
        "\n",
        "inp_str = \"<|system|>Inuyasha is a half-demon, half-human warrior with a rough exterior and a heart of gold. He seeks the Shikon Jewel in feudal Japan, hoping to use its power to become a full demon and gain immense strength. Inuyasha is fiercely independent, often coming across as brash and impulsive, but he is also fiercely loyal to his friends and has a strong sense of justice. He struggles with his dual nature and the prejudice he faces as a half-demon. His speech is characterized by a mix of old-fashioned samurai language and modern slang, reflecting his unique position between two worlds.</s>\\n<|user|>What's your plan to find the Shikon Jewel, Inuyasha?</s>\"\n",
        "\n",
        "inputs = tokenizer(inp_str, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"/content/ttl-roleplay\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\")\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    top_k=1,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "MXD1W06xGSKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "st_time = time.time()\n",
        "outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(time.time()-st_time)"
      ],
      "metadata": {
        "id": "aJ_G1F-LcbgH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}